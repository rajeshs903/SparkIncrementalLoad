spark-submit --packages com.databricks:spark-avro_2.11:4.0.0 --class com.poc.spark.InboundToHive /home/rajeshs/jars_from_intellij/new_jars/poc_hivetohbase_2.11-0.1.jar rajeshs_task_db retail_invoice_incr_avro test

SPARK_MAJOR_VERSION is set to 2, using Spark2
Ivy Default Cache set to: /home/rajeshs/.ivy2/cache
The jars for the packages stored in: /home/rajeshs/.ivy2/jars
:: loading settings :: url = jar:file:/usr/hdp/2.6.5.0-292/spark2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
com.databricks#spark-avro_2.11 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
        confs: [default]
        found com.databricks#spark-avro_2.11;4.0.0 in central
        found org.slf4j#slf4j-api;1.7.5 in central
        found org.apache.avro#avro;1.7.6 in central
        found org.codehaus.jackson#jackson-core-asl;1.9.13 in central
        found org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central
        found com.thoughtworks.paranamer#paranamer;2.3 in central
        found org.xerial.snappy#snappy-java;1.0.5 in central
        found org.apache.commons#commons-compress;1.4.1 in central
        found org.tukaani#xz;1.0 in central
:: resolution report :: resolve 1486ms :: artifacts dl 7ms
        :: modules in use:
        com.databricks#spark-avro_2.11;4.0.0 from central in [default]
        com.thoughtworks.paranamer#paranamer;2.3 from central in [default]
        org.apache.avro#avro;1.7.6 from central in [default]
        org.apache.commons#commons-compress;1.4.1 from central in [default]
        org.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]
        org.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]
        org.slf4j#slf4j-api;1.7.5 from central in [default]
        org.tukaani#xz;1.0 from central in [default]
        org.xerial.snappy#snappy-java;1.0.5 from central in [default]
        :: evicted modules:
        org.slf4j#slf4j-api;1.6.4 by [org.slf4j#slf4j-api;1.7.5] in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   10  |   1   |   1   |   1   ||   9   |   0   |
        ---------------------------------------------------------------------

:: problems summary ::
:::: ERRORS
        unknown resolver null


:: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS
:: retrieving :: org.apache.spark#spark-submit-parent
        confs: [default]
        0 artifacts copied, 9 already retrieved (0kB/8ms)
19/03/17 06:53:33 INFO SparkContext: Running Spark version 2.3.0.2.6.5.0-292
19/03/17 06:53:33 INFO SparkContext: Submitted application: com.poc.spark.InboundToHive
19/03/17 06:53:33 INFO SecurityManager: Changing view acls to: rajeshs
19/03/17 06:53:33 INFO SecurityManager: Changing modify acls to: rajeshs
19/03/17 06:53:33 INFO SecurityManager: Changing view acls groups to:
19/03/17 06:53:33 INFO SecurityManager: Changing modify acls groups to:
19/03/17 06:53:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(rajeshs); groups with view permissions: Set(); users  with modify permissions: Set(rajeshs); groups with modify permissions: Set()
19/03/17 06:53:34 INFO Utils: Successfully started service 'sparkDriver' on port 43694.
19/03/17 06:53:34 INFO SparkEnv: Registering MapOutputTracker
19/03/17 06:53:34 INFO SparkEnv: Registering BlockManagerMaster
19/03/17 06:53:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/03/17 06:53:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/03/17 06:53:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1d679863-f6b1-4c34-81ff-17c3585793d6
19/03/17 06:53:34 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
19/03/17 06:53:34 INFO SparkEnv: Registering OutputCommitCoordinator
19/03/17 06:53:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
19/03/17 06:53:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
19/03/17 06:53:34 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
19/03/17 06:53:34 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
19/03/17 06:53:34 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.
19/03/17 06:53:34 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.
19/03/17 06:53:34 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.
19/03/17 06:53:34 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.
19/03/17 06:53:34 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.
19/03/17 06:53:34 INFO Utils: Successfully started service 'SparkUI' on port 4049.
19/03/17 06:53:34 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://gw02.itversity.com:4049
19/03/17 06:53:34 INFO SparkContext: Added JAR file:///home/rajeshs/.ivy2/jars/com.databricks_spark-avro_2.11-4.0.0.jar at spark://gw02.itversity.com:43694/jars/com.databricks_spark-avro_2.11-4.0.0.jar with timestamp 1552820014915
19/03/17 06:53:34 INFO SparkContext: Added JAR file:///home/rajeshs/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar at spark://gw02.itversity.com:43694/jars/org.slf4j_slf4j-api-1.7.5.jar with timestamp 1552820014916
19/03/17 06:53:34 INFO SparkContext: Added JAR file:///home/rajeshs/.ivy2/jars/org.apache.avro_avro-1.7.6.jar at spark://gw02.itversity.com:43694/jars/org.apache.avro_avro-1.7.6.jar with timestamp 1552820014916
19/03/17 06:53:34 INFO SparkContext: Added JAR file:///home/rajeshs/.ivy2/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar at spark://gw02.itversity.com:43694/jars/org.codehaus.jackson_jackson-core-asl-1.9.13.jar with timestamp 1552820014916
19/03/17 06:53:34 INFO SparkContext: Added JAR file:///home/rajeshs/.ivy2/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar at spark://gw02.itversity.com:43694/jars/org.codehaus.jackson_jackson-mapper-asl-1.9.13.jar with timestamp 1552820014916
19/03/17 06:53:34 INFO SparkContext: Added JAR file:///home/rajeshs/.ivy2/jars/com.thoughtworks.paranamer_paranamer-2.3.jar at spark://gw02.itversity.com:43694/jars/com.thoughtworks.paranamer_paranamer-2.3.jar with timestamp 1552820014916
19/03/17 06:53:34 INFO SparkContext: Added JAR file:///home/rajeshs/.ivy2/jars/org.xerial.snappy_snappy-java-1.0.5.jar at spark://gw02.itversity.com:43694/jars/org.xerial.snappy_snappy-java-1.0.5.jar with timestamp 1552820014916
19/03/17 06:53:34 INFO SparkContext: Added JAR file:///home/rajeshs/.ivy2/jars/org.apache.commons_commons-compress-1.4.1.jar at spark://gw02.itversity.com:43694/jars/org.apache.commons_commons-compress-1.4.1.jar with timestamp 1552820014917
19/03/17 06:53:34 INFO SparkContext: Added JAR file:///home/rajeshs/.ivy2/jars/org.tukaani_xz-1.0.jar at spark://gw02.itversity.com:43694/jars/org.tukaani_xz-1.0.jar with timestamp 1552820014917
19/03/17 06:53:34 INFO SparkContext: Added JAR file:/home/rajeshs/jars_from_intellij/new_jars/poc_hivetohbase_2.11-0.1.jar at spark://gw02.itversity.com:43694/jars/poc_hivetohbase_2.11-0.1.jar with timestamp 1552820014917
19/03/17 06:53:34 INFO Executor: Starting executor ID driver on host localhost
19/03/17 06:53:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60545.
19/03/17 06:53:34 INFO NettyBlockTransferService: Server created on gw02.itversity.com:60545
19/03/17 06:53:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/03/17 06:53:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, gw02.itversity.com, 60545, None)
19/03/17 06:53:35 INFO BlockManagerMasterEndpoint: Registering block manager gw02.itversity.com:60545 with 366.3 MB RAM, BlockManagerId(driver, gw02.itversity.com, 60545, None)
19/03/17 06:53:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, gw02.itversity.com, 60545, None)
19/03/17 06:53:35 INFO BlockManager: external shuffle service port = 7447
19/03/17 06:53:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, gw02.itversity.com, 60545, None)
19/03/17 06:53:36 INFO EventLoggingListener: Logging events to hdfs:/spark2-history/local-1552820014935
inside the main
database_name = rajeshs_task_db
table_name = retail_invoice_incr_avro
*****isNewLoadRequested is false****
19/03/17 06:53:36 INFO SharedState: loading hive config file: file:/etc/spark2/2.6.5.0-292/0/hive-site.xml
19/03/17 06:53:36 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/rajeshs/spark-warehouse').
19/03/17 06:53:36 INFO SharedState: Warehouse path is 'file:/home/rajeshs/spark-warehouse'.
19/03/17 06:53:37 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/03/17 06:53:37 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
19/03/17 06:53:38 INFO metastore: Trying to connect to metastore with URI thrift://nn01.itversity.com:9083
19/03/17 06:53:38 INFO metastore: Connected to metastore.
19/03/17 06:53:53 INFO SessionState: Created local directory: /tmp/cd518151-7da7-4a75-adc2-7a4399a3c48a_resources
19/03/17 06:53:53 INFO SessionState: Created HDFS directory: /tmp/hive/rajeshs/cd518151-7da7-4a75-adc2-7a4399a3c48a
19/03/17 06:53:53 INFO SessionState: Created local directory: /tmp/rajeshs/cd518151-7da7-4a75-adc2-7a4399a3c48a
19/03/17 06:53:53 INFO SessionState: Created HDFS directory: /tmp/hive/rajeshs/cd518151-7da7-4a75-adc2-7a4399a3c48a/_tmp_space.db
19/03/17 06:53:53 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is file:/home/rajeshs/spark-warehouse
 The available files at inbound location are
2010-12-01.csv
2010-12-02.csv
2010-12-03.csv
picking latest file from inbound
latest file name : 2010-12-03.csv
filename for reading dataframe 2010-12-03.csv
+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+--------+
|InvoiceNo|StockCode|Description                    |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |datestr |
+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+--------+
|536847   |22155    |STAR DECORATION RUSTIC         |48      |2010-12-03 09:31:00|0.42     |17135.0   |United Kingdom|20101203|
|536847   |21807    |WHITE CHRISTMAS STAR DECORATION|36      |2010-12-03 09:31:00|0.42     |17135.0   |United Kingdom|20101203|
+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+--------+
only showing top 2 rows

current_partition 20101203 and fileValue 20101203
file was already processed
file is already processed...
2010-12-03.csv is already loaded to table
 Please add optional parameter 'request_load' at the end of spark-submit to load the new file to inbound
current status of data is
+--------+-----+
| datestr|count|
+--------+-----+
|20101202| 8436|
|20101203| 2202|
|20101201| 3108|
+--------+-----+

total number of records :13746
total number of unique records :7317
******job is completed******
